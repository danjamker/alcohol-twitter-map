{"ts":1378424677645,"silentsave":true,"restoring":false,"patch":[[{"diffs":[[1,"extends layout\nblock content\n    h1 About this project\n    p Monitoring the rates of consumption of alcohol across the UK is a timely problem with an ever-increasing consumption levels, and calls from public service about the effect it is having on people and society. However methods that are currently utilised are costly, time consuming, and donâ€™t supply detailed enough results as they look at user  rather than user patters of drinking. In this paper we look into the abilities of using Twitter (a popular micro blogging site) to monitor rate of alcohol consumption in regions across the UK. Looking into variation of term usage, along with trends within a specific key word set. This study was performed over a 2 week windows; analyzing textual markers in over 17 millon tweets all geo located within the UK. A score was given to each tweet based on the number of markers used and the sum of all markers in the tweet. Scores form different geo-locations sets and time granularities were compared to the ground truth data from the Health & Social Care Information Centre (HSCIC) weekly alcohol consumption pattern. We managed to get high linear correlations between the ground truth and the twitter alcohol score, with highs of 0.87 with a p-value of less than 0.01 for regions in the UK. The near real-time monitoring of alcohol-consumption, and the limited overheads that the use of social media incurs, means that such a method could be used to inform decisions that in the past have relied purely on slow and laborious data collection methods (e.g. questionnaires). Different variations in language were detected over time and regions, with lags in certain words been detected, e.g. 'drunk' appearing at midnight, and 'hungover' appearing from around midday.\n\n    h1 Implementation\n    ul\n        li Map - The map was one of the more ticker parts of the system to impliment. This was implimented by using \n        a(href='http://www.polymaps.com/') Polymaps \n        for the UI and the background maps. Over this was layers a GeoJSON vector maps, for both a national and regonal scales, however for local map which depected the boundries of each postcode there where so many vectores that the file was over 17Mb. For this reason each vectore was imported into a \n        a(href='http://www.mongobd.com') MongoDB \n        hosted on \n        a(href='http://www.mongolab.com') MongoLab\n        . The reson for using a MongoDB soultion was the abbility to index all the vector points and used them for searching. This mean that from the browser a message is sent to the server saying what the browser can see, then the server only return what is within the view of polymaps. This reduced the size of each packet. The original files can be download from here\n        ul\n            li \n                a(href=\"/National.json\") Natinoal\n            li \n                a(href=\"/Regonal.json\") Regonal\n\n        li The server was writen in Node.js using the Express frame work for routing and rendering of pages. The theme and structure of each pages was based on the \n        a(href='http://www.getbootstrap.com') Bootstrap \n        and the charts where implimentes with the \n        a(href='https://github.com/nnnick/Chart.js') Chart.js \n        Both of which made the development of the website quick and simple as my background is not in CSS of website layout. Hosting was provided by \n        a(href='http://www.heroku.com') Heroku \n        which offer 36hrs free hositng a month, and allows for easy deplyment through git push, and fully suports the Node.js system. \n"]],"start1":0,"start2":0,"length1":0,"length2":3574}]],"length":3574}
{"contributors":[],"silentsave":false,"ts":1378462815626,"patch":[[{"diffs":[[0,"inking. "],[1,"\n    p "],[0,"In this "]],"start1":434,"start2":434,"length1":16,"length2":23},{"diffs":[[0,"the UK. "],[1,"\n    p "],[0,"The near"]],"start1":1293,"start2":1293,"length1":16,"length2":23},{"diffs":[[0,"ystem. \n"],[1,"\n        li Data Processing was impliment in \n        a(href=\"http://hadoop.apache.org/\")Haddop \n        | which iterated over the 17 million tweets. The map andredce part was writen in \n        a(href=\"http://www.python.org\")Python \n        | to usterlise the NLTK framework for text analises, which alloed for the extraction of collocation, and removal of stop word. Sample of all the code can be found in my \n        a(href=\"http://www.github.com/danjamker\") Github\n"]],"start1":3580,"start2":3580,"length1":8,"length2":477}]],"length":4057,"saved":false}
